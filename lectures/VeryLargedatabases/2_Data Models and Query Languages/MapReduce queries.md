Powerful, parallel data processing jobs that split big data tasks into two phases ([[Map]] and [[Reduce]]) to run across clusters, allowing for complex [[filtering]], [[aggregation]] (like counts, sums), and [[transformations]] (e.g., word counts, log analysis) on massive datasets faster than traditional methods, often leveraging frameworks like [[Hadoop]] or [[NoSQL]] databases. Developers write simple map (key-value pair generation) and reduce (aggregation) functions, and the framework handles distribution, fault tolerance and data movement. 